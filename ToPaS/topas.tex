%Формат файла
\documentclass[12pt]{article} 
\usepackage[paperheight=297mm,
   paperwidth=210mm,
   top=20mm,
   bottom=20mm,
   left=15mm,
   right=15mm]{geometry}


%Текст
\usepackage[fontsize=12pt]{fontsize}
\usepackage[russian]{babel}
\usepackage{color}
\usepackage{transparent}
\usepackage{amsthm}
\usepackage{multicol}
\parindent=0cm

\theoremstyle{definition}
\newtheorem{theorem}{Теорема}[section]
\newtheorem*{example}{Пример}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{definition}{Определение}
\newtheorem{statement}[theorem]{Утверждение}
\newtheorem{consequence}{Следствие}[subsection]
\renewcommand\qedsymbol{$\blacksquare$}


%Картинки
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary {arrows.meta}

%Математика
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[output-decimal-marker={,}]{siunitx}

%Всякое
\usepackage{relsize}
\usepackage{enumerate}
\usepackage[inline]{enumitem}
\usepackage{hyperref}

%Мат команды
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\nd}{\mathcal{N}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

%Оглавление
\title{\textbf{ТВиМС}}\date{}\author{}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newcommand\defeq{\stackrel{\mathclap{\scalebox{0.6}{def}}}{=}}

\begin{document}

\maketitle
\tableofcontents
\label{toc}
\newpage

\section{Случайные величины}

\begin{definition}
    Случайной величиной $\xi$ называется функция, заданная на множестве $\Omega$, принимающая значения в $\R$.
\end{definition}

    Задать случайную величину, значит указать все ее реализации и соответственные вероятности.

\begin{definition}
    Индикатором события $A$ называется случайная величина: $$\I(A)\sim \left(\genfrac{}{}{0pt}{0}{0}{1-\prob(A)}\,\,\, \genfrac{}{}{0pt}{0}{1}{\prob(A)} \right)$$
\end{definition}

\begin{definition}
    Законом распределения случайной величины называется некоторое правило, позволяющее однозначно определить значение вероятности по значению случайной величины.
\end{definition}

    \subsection{Числовые характеристики случайных величин}

\begin{definition}
    Математическим ожиданием дискретной случайной величины, если оно существует, называется число: 
    $$\E (\xi)=\sum_{i=1}^{n}\omega_i\cdot \prob(\xi=\omega_i)$$
\end{definition}

\begin{definition}
    Дисперсией случайной величины называется $\D(\xi)=\E(\xi-\E(\xi))^2$.
\end{definition}

\begin{theorem}
    $\E(a\xi+b\eta+c)=a\E(\xi)+b\E(\xi)+c;\;a,b,c\in \R$
\end{theorem}

\begin{proof}
    \begin{align*}
        &\E(a\xi+b\eta+c)=\\
        =&\sum_{i=1}^n \widehat{\omega}_i\cdot\prob(a\xi+b\eta+c=\widehat{\omega}_i)=\\
        =&c+\sum_{i=1}^n \widehat{\omega}_i^c\cdot\prob(a\xi+b\eta=\widehat{\omega}_i^c)=\\
        =&c+\sum_{i=1}^n \omega_i^\xi\cdot \prob(a\xi=\omega_i^\xi)+\sum_{i=1}^n \omega_i^\eta\cdot\prob(b\eta=\omega_i^\eta)=\\            =&c+a\E(\xi)+b\E(\eta)
    \end{align*}
\end{proof}

\begin{theorem}
    Дисперсия случайной величины $\xi$ может быть вычислена, как $\D(\xi)=\E(\xi^2)-(\E(\xi))^2$
\end{theorem}

\begin{proof}
    \begin{align*}
        &\D(\xi)=\E(\xi-\E(\xi))^2=\\
        =&\E(\xi^2-2\xi\E(\xi)+(\E(\xi))^2)=\\
        =&\E(\xi^2)-2(\E(\xi))^2+(\E(\xi))^2=\\
        =&\E(\xi^2)-(\E(\xi))^2
    \end{align*}
\end{proof}

\begin{definition}
    Стандартным отклонением случайной величины $\xi$ называется $\sigma(\xi)=\sqrt{\D(\xi)}$.
\end{definition}

    \subsubsection{Распределение Бернулли}

\begin{definition}
    Случайная величина $\xi$  распределена по Бернулли, если ее распределение суть индикатор.
    $$Ber(p)\sim \left(\genfrac{}{}{0pt}{0}{0}{1-p} \,\,\, \genfrac{}{}{0pt}{0}{1}{p} \right)$$
\end{definition}

    \subsubsection{Биномиальное распределение}

\begin{definition}
    Случайная величина $\xi$ распределена биномиально, если она моделирует схему испытаний  Бернулли или является суммой бернуллиевых случайных величин.
    $$B(p,\,n)\sim \left( \genfrac{}{}{0pt}{0}{0}{(1-p)^n} \,\,\, \genfrac{}{}{0pt}{0}{\cdots}{\cdots} \,\,\, \genfrac{}{}{0pt}{0}{k}{C_n^kp^k(1-p)^{n-k}} \,\,\, \genfrac{}{}{0pt}{0}{\cdots}{\cdots} \,\,\, \genfrac{}{}{0pt}{0}{n}{p^n} \right)$$
\end{definition}

\begin{theorem}
    Математическое ожидание биномиально распределенной случайной величины $\xi$ может быть вычислено, как $\E(\xi)=np$.
\end{theorem}

\begin{proof}
    \begin{align*}
        &\E(\xi) = C_n^1pq^{n-1} + 2C_n^2p^2q^{n-2} + \ldots + kC_n^kp^kq^{n-k} + \ldots + nC_n^np^n = \\
        =& np \cdot (C_{n-1}^0q^{n-1} + C_{n-1}^1pq^{n-2}+\ldots + C_{n-1}^{k-1}p^{k-1}q^{n-k} + \ldots + C_{n-1}^{n-1}p^{n-1})=\\
        =& np \cdot (q+p)^{n-1}=\\
        =& np
    \end{align*}
\end{proof}

\begin{theorem}
    Дисперсия независимых случайных величин линейна: $\D(\xi+\eta)=\D(\xi)+\D(\eta)$
\end{theorem}

\begin{lemma}
    Дисперсия биномиально распределенной случайной величины $\xi$ может быть вычислена, как $\D(\xi)=npq$.
\end{lemma}

\begin{proof}
    Пусть $\eta$ -- число успехов в одном испытании Бернулли. Тогда:
    $$\eta \sim B(p,\,1) \sim \left(\genfrac{}{}{0pt}{0}{0}{q} \,\,\, \genfrac{}{}{0pt}{0}{1}{p}\right)$$
    В таком случае $\D(\eta)=\E(\eta^2)-(\E(\eta))^2=p-p^2=pq$. Тогда по теореме 1.4:
    $$\D(\xi)=\sum_{i=1}^{n}\D(\xi_i)=pq \cdot n = npq$$
\end{proof}

\subsubsection{Геометрическое распределение}

\begin{definition}
    Случайная величина $\xi$ распределена геометрически, если она моделирует схему испытаний до первого успеха с вероятностью $p$.
    $$Geom(p) \sim \left( \genfrac{}{}{0pt}{0}{1}{p} \,\,\, \genfrac{}{}{0pt}{0}{2}{qp} \,\,\, \genfrac{}{}{0pt}{0}{\cdots}{\cdots} \,\,\, \,\,\, \genfrac{}{}{0pt}{0}{n}{q^{n-1}p} \,\,\, \genfrac{}{}{0pt}{0}{\cdots}{\cdots}\right)$$
\end{definition}

\begin{lemma}
    Математическое ожидание геометрически распределенной случайной величины $\xi$ может быть вычислено, как $\E(\xi)=\dfrac{1}{p}$.
\end{lemma}

\begin{proof}
    \begin{align*}
        &\E(\xi)=p+2qp+2q^2p+\ldots+kq^{k-1}p+\ldots=\\
        =&(p+qp+q^2p+\ldots+q^{k-1}p+\ldots)+(qp+2q^2p+\ldots+(k-1)q^{k-1}p+\ldots)=\\
        =&\frac{p}{1-q}+q(p+2pq+\ldots+(k-1)q^{k-2}p+\ldots)\\\\
        &\E(\xi)=1+q\E(\xi)\\
        &\E(\xi)(1-q)=1\\
        &\E(\xi)=\frac{1}{p}
    \end{align*}
\end{proof}

\begin{lemma}
    Дисперсия геометрически распределенной случайной величины $\xi$ может быть вычислена, как $\D(\xi)=\dfrac{q}{p^2}$.
\end{lemma}

\begin{proof}
    \begin{align*}
        &\D(\xi)=\E(\xi^2)-(\E(\xi))^2\\\\
        &\E(\xi^2)=p+2qp+9q^2p+\ldots+k^2q^{k-1}p+\ldots=\\
        =&p+qp+3qp+4q^2p+5q^2p+\ldots=\\
        =&(qp+4q^2p+\ldots)+(p+3qp+5q^2p+\ldots)\\\\
        &\E(\xi^2)=q\E(\xi^2)+\E(2\xi-1)=\\
        =&(1-p)\E(\xi^2)+\frac{2}{p}-1=\frac{2-p}{p^2}\\\\
        &\D(\xi)=\frac{2-p}{p^2}-\frac{1}{p^2}=\frac{q}{p^2}
    \end{align*}
\end{proof}

\subsubsection{Гипергеометрическое распределение}
\begin{definition}
    Случайная величина $\xi$ распределена гипергеометрически, если она моделирует выбор $n$ элементов из множества мощности $N$ с $K$ помеченными и является числом помеченных в выборке.
\end{definition}
$$\xi \sim HG(N,\,K,\,n)$$
$$\prob(\xi=k)=\frac{C_K^k\cdot C_{N-K}^{n-k}}{C_N^n}$$

\begin{statement}
    Математическое ожидание гипергеометрически распределенной случайной величины $\xi$ может быть вычислено, как $\E(\xi)=\dfrac{n\cdot K}{N}$.
\end{statement}

\begin{proof}
    \begin{align*}
    &\xi = \I(A_1)+\I(A_2)+\ldots+\I(A_n),\, \text{где}\, A_i=\{i\text{-ый элемент выборки помечен}\}\\
    &\I(A_i)\sim \left(\genfrac{}{}{0pt}{0}{0}{1-\frac{K}{N}} \,\,\, \genfrac{}{}{0pt}{0}{1}{\frac{K}{N}} \right)\\
    &\E(\xi)=\sum_{i=1}^n \E(\I(A_i))=n\cdot \frac{K}{N}
\end{align*}
\end{proof}

\subsubsection{Распределение Паскаля}

\begin{definition}
    Случайная величина $\xi$ распределена по Паскалю, если она моделирует испытания до первых $k$ успехов.
\end{definition}

\begin{definition}
    $$\xi \sim NB(p,\,k)\text{, если }\xi=\sum_{i=1}^k\eta_i:\,\forall i \in \{1,\,2,\,\ldots\,,\,k\}:\,\eta_i\sim Geom(p)$$
    $$\prob(\xi=n)=C_{n-1}^{k-1}p^kq^{n-k}$$
\end{definition}

\begin{statement}
    Математическое ожидание случайной величины $\xi$, распределенной по Паскалю, может быть вычислено, как $\E(\xi)=\dfrac{k}{p}$.
\end{statement}
\begin{proof}
    Поскольку математическое ожидание линейно: 
    $$\E(\xi)=\sum_{i=1}^{k}\E(\xi_i)=\frac{1}{p}\cdot k =\frac{k}{p}$$
\end{proof}

\section{Ковариация}

\begin{definition}
    Пусть $\xi$ и $\eta$ -- случайные величины, тогда ковариацией называется:
    $$cov(\xi;\,\eta)=\E((\xi-\E(\xi))(\eta-\E(\eta)))$$
\end{definition}
\begin{theorem}
    Для $cov(\xi;\,\eta)$ выполняются свойства:
    \begin{align*}
        1.\,\,&cov(\xi;\,\xi)\geq0\\
        2.\,\,&cov(\xi;\,\eta)=cov(\eta;\,\xi)\\
        3.\,\,&cov(\lambda \xi;\,\eta)=\lambda\cdot cov(\xi;\,\eta)\\
        4.\,\,&cov(\xi_1+\xi_2;\,\eta)=cov(\xi_1;\,\eta)+cov(\xi_2;\,\eta)\\
        5.\,\,&cov(\xi;\,\eta)\leq \D(\xi)\cdot\D(\eta)
    \end{align*}
\end{theorem}
\begin{theorem}
    $$cov(\xi;\,\eta)=\E(\xi\cdot\eta)-\E(\xi)\cdot\E(\eta)$$
\end{theorem}
\begin{proof}
    \begin{align*}
        &\E((\xi-\E(\xi))(\eta-\E(\eta)))=\\
        =&\E(\xi\cdot\eta-\xi\E(\eta)-\eta\E(\xi)+\E(\xi)\cdot\E(\eta))=\\
        =&\E(\xi\cdot\eta)-\E(\xi\E(\eta))-\E(\eta\E(\xi))+\E(\xi)\cdot\E(\eta)=\\
        =&\E(\xi\cdot\eta)-\E(\xi)\cdot\E(\eta)
    \end{align*}
\end{proof}
\begin{theorem}
    $$\D(\xi+\eta)=\D(\xi)+\D(\eta)+2\cdot cov(\xi;\,\eta)$$
\end{theorem}
\section{Корреляция}

\begin{definition}
    Пусть $\xi$ и $\eta$ -- случайные величины: $\D(\xi)\neq0,\,\D(\eta)\neq0,\,cov(\xi;\,\eta)$ определена корректно. Тогда коэффициентом корреляции $\xi$ и $\eta$ называется:
    $$corr(\xi;\,\eta)=r_{\xi\eta}=\frac{cov(\xi;\,\eta)}{\sigma(\xi)\cdot\sigma(\eta)}$$
    Свойства:
    \begin{align*}
        1.\,\,&|r_{\xi\eta}|\leq 1\\
        2.\,\,&|r_{\xi\eta}|=1\Longleftrightarrow \exists\, k\neq 0,\,b:\,\eta=k\xi+b\text{ (почти наверное).}
    \end{align*}
\end{definition}
\section{Мера Жордана}
\begin{definition}
    $A$ измеримо по Жордану, если $\mu^j(A)=\mu_j(A)$, где $\mu^j(A)=\inf\{\mu(\delta):\,A\subset \delta\}$, $\mu_j(A)=\sup\{\mu(\delta):\,\delta \subset A\}$.
\end{definition}
\begin{definition}
    Пусть $A\subset \Omega$, тогда $\prob(x\in A)=\dfrac{\mu(A)}{\mu(\Omega)}$.
\end{definition}

\section{Распределение Пуассона}
\begin{theorem}[Теорема Пуассона]
    Пусть $n\to\infty$, $p\to 0$, $np\to \lambda$, $\lambda=\text{const}$, тогда если $\xi$ -- количество успехов в серии испытаний Бернулли, то она распределена по Пуассону:
    $$\xi \sim P(\lambda):\,\,\prob(\xi=k)=\dfrac{\lambda^k}{k!}e^{-\lambda}$$
\end{theorem}
\begin{proof}
    \begin{align*}
        &P(\xi=k)=C_n^kp^kq^{n-k}\to\\
        \to&\dfrac{n!}{(n-k)!\cdot k!}\cdot p^kq^{n-k}\to\\
        \to&\dfrac{p^k}{k!\cdot q^k}\cdot \dfrac{n!}{(n-k)!}\cdot q^n\to\\
        \to&\dfrac{p^kq^n}{k!\cdot q^k}\cdot n\cdot(n-1)\cdot(n-2)\cdot\ldots\cdot(n-k+1)\to\\
        \to&\dfrac{p^kq^nn^k}{k!\cdot q^k}\cdot 1\cdot\left(1-\dfrac{1}{n}\right)\cdot\left(1-\dfrac{2}{n}\right)\cdot\ldots\cdot\left(1-\dfrac{k-1}{n}\right)\to
    \end{align*}
    \begin{align*}
        \to&\dfrac{\lambda^k\cdot q^n}{k!\cdot q^k}\cdot\left(1-\dfrac{1}{n}\right)\cdot\left(1-\dfrac{2}{n}\right)\cdot\ldots\cdot\left(1-\dfrac{k-1}{n}\right)\to\\
        \to&\dfrac{\lambda^k}{k!}q^n
    \end{align*}
    $$\ln q^n= n\cdot \ln(1-p)\to -np\to-\lambda \Longrightarrow \dfrac{\lambda^k}{k!}q^n = \dfrac{\lambda^k}{k!}e^{-\lambda}$$
\end{proof}
\begin{theorem}
    $$\lim_{n\to\infty}\sum_{i=0}^n\dfrac{x^i}{i!}=e^x,\,\,x\in \R$$
\end{theorem}
\begin{theorem}
    Пусть $\xi\sim P(\lambda)$. Тогда $\E(\xi)=\D(\xi)=\lambda$.
\end{theorem}
\begin{proof}
    $$\E(\xi)=\sum_{k=0}^{\infty}k\dfrac{\lambda^k}{k!}e^{-\lambda}=e^{-\lambda}\sum_{k=0}^{\infty}\dfrac{\lambda^k}{(k-1)!}=e^{-\lambda}\cdot\lambda\sum_{k=0}^{\infty}\dfrac{\lambda^{k-1}}{(k-1)!}=e^{-\lambda}\cdot\lambda\cdot e^\lambda=\lambda$$
\end{proof}
\begin{lemma}
    Пусть $\xi\sim P(\lambda_\xi)$, $\eta\sim P(\lambda_\eta)$, $\xi$ и $\eta$ независимы. Тогда $(\xi+\eta)\sim P(\lambda_\xi+\lambda_\eta)$.
\end{lemma}
\begin{proof}
    \begin{align*}
        &\prob(\xi+\eta=n)=\\
        =&\sum_{i=0}^{n}\prob(\xi=i)\cdot\prob(\eta=n-i)=\\
        =&\sum_{i=0}^{n}\dfrac{\lambda_\xi^i}{i!}\cdot e^{-\lambda_\xi}\cdot \dfrac{\lambda_\eta^{n-i}}{(n-i)!}\cdot e^{-\lambda_\eta}=\\
        =&e^{-(\lambda_\xi+\lambda_\eta)}\sum_{i=0}^{n}\dfrac{\lambda_\xi^i\cdot\lambda_\eta^{n-i}}{i!\cdot(n-i)!}\cdot \dfrac{n!}{n!}=\\
        =&\dfrac{e^{-(\lambda_\xi+\lambda_\eta)}}{n!}\sum_{i=0}^{n}C_n^i\lambda_\xi^i\lambda_\eta^{n-i}=\\
        =&e^{-(\lambda_\xi+\lambda_\eta)}\cdot \dfrac{(\lambda_\xi+\lambda_\eta)^n}{n!}
    \end{align*}
\end{proof}

\section{Ветвящиеся процессы}

\subsection{Цепи Маркова}

\begin{definition}
    Последовательность случайных величин $\xi_0,\,\xi_1,\,\ldots\,,\,\xi_n$ называется Цепью Маркова, если 
    $$\forall n,\,i_0,\,i_1,\,\ldots\,,\,i_n:\,\,\prob(\xi_{n-1}=x_{i_{n-1}},\,\ldots\,,\,\xi_0=x_{i_{0}})$$
    верно, что:
    $$\prob(\xi_{n}=x_{i_{n}}\,|\,\xi_{n-1}=x_{i_{n-1}},\,\ldots\,,\,\xi_0=x_{i_{0}})=\prob(\xi_{n}=x_{i_{n}}\,|\,\xi_{n-1}=x_{i_{n-1}})$$
\end{definition}
\begin{definition}
    Цепь Маркова называется однородной, если:
    $$\forall i,\,j:\,\,\prob(\xi_n=x_j\,|\,\xi_{n-1}=x_i)=p_{i,j}\text{ не зависит от }n.$$
\end{definition}
\begin{definition}
    Матрица $A=(a_{i,j})$ называется стохастической, если:
    $$\forall i,\,j:\,\,a_{i,j}\in [0;\,1],\,\,\sum_{i}(a_{i,j})=1$$
\end{definition}
\begin{definition}
    Матрица $\pi=(p_{i,j})$ называется матрицей переходных вероятностей.
\end{definition}
\begin{theorem}
    Пусть $p^{(0)}=(p^{(0)}_1,\,p^{(0)}_2,\,\ldots\,,\,p^{(0)}_n)$ и $p^{(k)}=(p^{(k)}_1,\,p^{(k)}_2,\,\ldots\,,\,p^{(k)}_n)$ -- начальное распределение и распределение на $k$-ом шаге соответственно вероятностей Марковской цепи, где $p^{(k)}_i=\prob(\xi_k=x_i)$. Тогда:
    $$p^{(k)}=p^{(0)}\cdot\pi^k$$
\end{theorem}
\subsubsection{Классификация состояний Марковских цепей}
\begin{definition}
    Состояние $x_j$ достижимо из $x_i$, если:
    $$\exists\,k:\,\,P^k_{ij}=\prob(\xi_{m+k}=x_j\,|\,\xi_m=x_i)>0$$
\end{definition}
\begin{definition}
    Состояния называются сообщающимися, если они достижимы друг для друга.
\end{definition}
\begin{definition}
    Состояние $x_i$ называется несущественным, если существует такое состояние $x_j$, что $x_j$ достижимо из $x_i$, но $x_i$ недостижимо из $x_j$.
\end{definition}
\begin{definition}
    Состояние $x_i$ называется существенным, если существует такое состояние $x_j$, что $x_j$ достижимо из $x_i$ и $x_i$ достижимо из $x_j$.
\end{definition}
\begin{definition}
    Марковская цепь, все состояния которой составляют один класс сообщающихся состояний, называется неразложимой.
\end{definition}
\begin{definition}
    Состояние $x_i$ называется возвратным, если вероятность возвращения в это состояние равна 1.
\end{definition}
\begin{definition}
    Состояние $x_i$ называется невозвратным, если вероятность возвращения в это состояние не равна 1.
\end{definition}
\begin{definition}
    Возвратное состояние $x_i$ называется возвратным положительным, если среднее время возвращения в него конечно.
\end{definition}
\begin{definition}
    Возвратное состояние $x_i$ называется возвратным нулевым, если среднее время возвращения в него бесконечно.
\end{definition}
\begin{definition}
    Состояние $x_i$ называется периодическим, если $\NOD\{k:\,\,P^{(k)}_{ii}>0\}=d>1$, где $d$ -- период состояния.
\end{definition}
\subsubsection{Эргодичность}
\begin{definition}
    Марковская цепь называется эргодической, если:
    $$\forall i,\,j:\,\,\exists\lim_{k\to\infty}P^{(k)}_{ij}=p_{ij}>0,\,\,\sum_{j}p_j=1$$
\end{definition}
\begin{theorem}[Критерий эргодичности]
    Марковская цепь эргодична, если:
    $$\exists\,k:\,\,\forall i,\,j:\,\,P^{(k)}_{ij}>0$$
\end{theorem}

\subsection{Процесс Гальтона-Ватсона}

\begin{definition}
    Пусть $p_0,\,p_1,\,\ldots\,,\,p_m:\,p_m\geq 0;\,p_0+p_1+\ldots+p_m=1$ – начальное распределение. Пусть для $i\geq 2$ определено: $$p_i^{*k}=\sum_{i_1+i_2+\ldots+i_k=i}p_{i_1}\cdot p_{i_2}\cdot\ldots\cdot p_{i_k}$$
    Процесс Гальтона-Ватсона есть марковская цепь $Z(n),\, n\in \N_0$ с начальным распределением $P_0(k)=\prob(Z(0)=k)$ и переходными вероятностями:
    $$P_{ij}=\prob(Z(n+1)=j\,|\,Z(n)=i)=\begin{cases}
        p_j^{*i},\text{ если }i\geq 1,\,j\geq 0\\
        \delta_{0j},\text{ если }i\geq 0,\,j\geq 0
    \end{cases}$$
    Если не оговорено иного, $P_0(1)=\prob(Z(0)=1)=1;\,\,P_0(k)=0,\,\,k\neq1$.
\end{definition}
\begin{example}[Деление клетки 1]\label{Деление клетки 1}
    Рассмотрим популяцию частиц. Пусть после каждой единицы времени частица либо умирает, либо делится на двое. При этом пусть в начале мы имели только одну частицу, то есть $Z(0)=1$. Рассмотрим случайную величину $\xi_i^{(n)}$ – число потомков $i$-й частицы $n$-го поколения:
    $$Z(n+1)=\xi_1^{(n)}+\xi_2^{(n)}+\ldots+\xi_{Z(n)}^{(n)},\,\,n\geq0$$
    Заметим, что можно также фиксировать не число частиц в момент времени $n$, а число частиц первого поколения:
    $$Z(n+1)=Z_1(n)+Z_2(n)+\ldots+Z_{Z(1)}(n)$$
    Таким образом для независимых $\xi_i^{(n)}$ верно:
    $$\xi_1^{(n)}+\xi_2^{(n)}+\ldots+\xi_{Z(n)}^{(n)}\defeq Z(n+1)=Z_1(n)+Z_2(n)+\ldots+Z_{Z(1)}(n)$$
    Далее смотреть в примере \ref{Деление клетки 2}.
\end{example}

\subsection{Тождество Вальда}

\begin{theorem}
    Пусть $\xi_0,\,\xi_1,\,\ldots$ – независимые одинаково распределенные случайные величины. Пусть $\tau$ – случайный момент времени, не зависящий от $(\xi_i)$. Пусть $S_n=\xi_0+\xi_1+\ldots+\xi_n$. Тогда:
    $$\E(S_\tau)=\E(\xi)\cdot\E(\tau)$$ 
\end{theorem}
\begin{proof}
    $$\sum_{k=1}^{\infty}\prob(\tau\geq k)=\sum_{k=1}^{\infty}\sum_{n=1}^{k}\prob(\tau=n)=\sum_{n=1}^{\infty}\sum_{k=1}^{n}\prob(\tau=n)=\sum_{n=1}^{\infty}n\cdot\prob(\tau=n)=\E(\tau)$$
    $$\E(S_\tau)=\sum_{n=1}^{\infty}\E(S_\tau;\,\tau=n)=\sum_{n=1}^{\infty}\E(\xi_1+\xi_2\ldots+\xi_n;\,\tau=n)=\sum_{n=1}^{\infty}\sum_{k=1}^{n}\E(\xi_k;\,\tau=n)=$$
    $$=\sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\E(\xi_k;\,\tau=n)=\sum_{k=1}^{\infty}\E(\xi_k;\,\tau\geq k)=\sum_{k=i}^{\infty}\E(\xi_k)\cdot\E(\tau\geq k)=\E(\xi)\cdot\E(\tau)$$
\end{proof}

\section{Производящие функции}

\begin{definition}
    Производящей функцией произвольной последовательности $(a_n)$ называется выражение вида:
    $$a_0+a_1z+a_2z^2+\ldots=\sum_{i=0}^{\infty}a_iz^i$$
\end{definition}

\subsection{Операции с производящими функциями}

\begin{definition}
    Суммой производящих функций $A(z)=a_0+a_1z+a_2z^2+\ldots$ и $B(z)=b_0+b_1z+b_2z^2+\ldots$ называется производящая функция:
    $$A(z)+B(z)=(a_0+b_0)+(a_1+b_1)z+(a_2+b_2)z^2+\ldots$$
\end{definition}
\begin{definition}
    Произведением производящих функций $A(z)=a_0+a_1z+a_2z^2+\ldots$ и $B(z)=b_0+b_1z+b_2z^2+\ldots$ называется производящая функция:
    $$A(z)\cdot B(z)=a_0b_0+(a_0b_1+a_1b_0)z+(a_0b_2+a_1b_1+a_2b_0)z^2+\ldots$$
\end{definition}
\begin{definition}
    Пусть $A(z)=a_0+a_1z+a_2z^2+\ldots$; $B(t)=b_0+b_1t+b_2t^2+\ldots;\,b_0=0$ – производящие функции. Подстановкой производящей функции $B$ в производящую функцию $A$ будет называться производящая функция:
    $$A(B(t))=a_0+a_1b_1t+(a_1b_2+a_2b_1^2)t^2+(a_1b_3+2a_2b_1b_2+a_3b_1^3)t^3+\ldots$$
\end{definition}
\begin{theorem}
    Пусть $B(t)=b_0+b_1t+b_2t^2+\ldots;\,b_0=0;\,b_1\neq 0$ – производящая функция. Тогда существуют единственные такие функции $A(z)=a_0+a_1z+a_2z^2+\ldots;\,a_0=0$ и $C(u)=c_0+c_1u+c_2u^2=\ldots;\,c_0=0$, что $A(B(t))=t$ и $B(C(u))=u.$ Функция $A$ называется левой обратной, а функция $C$ – правой обратной к функции $B$.
\end{theorem}
\begin{proof}
    Рассмотрим левую обратную функцию:
    $$A(B(t))=a_1b_1t+(a_1b_2+a_2b_1^2)t^2+(a_1b_3+2a_2b_1b_2+a_3b_1^3)t^3+\ldots=t$$
    Чтобы равенство выполнялось, коэффициент при $t$ должен равняться 1, а коэффициенты при $t^n,\,n\geq 2$ должны равняться 0. Отсюда $a_1b_1=1\Longrightarrow a_1=\dfrac{1}{b_1}$. Пусть аналогично определены коэффициенты $a_1,\,a_2,\,\ldots\,,\,a_n$. Тогда коэффициент $a_{n+1}$ будет определяться из условия, что многочлен $a_{n+1}b^{n+1}_1+\ldots$ от $a_1,\,a_2,\,\ldots\,,\,a_n,\,a_{n+1}$ и $b_1,\,b_2,\,\ldots\,,\,b_n,\,b_{n+1}$, являющийся коэффициентом при $t^{n+1}$, будет равен нулю. Поскольку $b_1\neq 0$ по условию, получаем уравнение от $a_{n+1}$ с единственным корнем. То есть мы однозначно можем задать такие коэффициенты $a_1,\,a_2,\,\ldots$, чтобы $A(B(t))=t$.\bigskip
    
    Доказательство для правой обратной функции аналогично.
\end{proof}
\begin{definition}
    Производящая функция называется рациональной, если ее можно представить в виде $\dfrac{P(x)}{Q(x)}$, где $P(x)$ и $Q(x)$ – многочлены.
\end{definition}

\subsection{Производящие функции вероятности}

\begin{definition}
    Производящей функцией $\varphi_\xi$ случайной величины $\xi$ называется производящая функция последовательности $\left(\prob(\xi=n)\right)^\infty_{n=0}$:
    $$\varphi_\xi(z)=\sum_{n=0}^{\infty}z^n\prob(\xi=n)=\E(z^\xi)$$
    Причем $\varphi_\xi(1)=1$ как сумма вероятностей. Рассмотрим производную данной функции:
    \begin{multicols}{2}
        \centering
        \[\varphi'_\xi(z)=\sum_{n=0}^\infty n\cdot z^{n-1}\prob(\xi=n)\]\\
        \[\varphi'_\xi(1)=\sum_{n=0}^\infty n\cdot \prob(\xi=n)=\E(\xi)\]
    \end{multicols}
\end{definition}
\begin{definition}
    Итерацией производящей функции случайной величины порядка $n$ называется композиция, строящаяся рекуррентно:
    \begin{multicols}{3}
        \centering
        $\varphi_0(z)=z$\\
        $\varphi_1(z)=\varphi(z)$\\
        $\varphi_{n+1}(z)=\varphi_n(\varphi(z))$
    \end{multicols}
\end{definition}
\begin{definition}
    Пусть $\xi_i,\,\,i\in\{1,\,\,2,\ldots,\,\,n\}$ – независимые случайные величины, тогда:
    $$\varphi_{\xi_1+\xi_2+\ldots+\xi_n}(z)=\varphi_{\xi_1}\cdot\varphi_{\xi_2}\cdot\ldots\cdot\varphi_{\xi_n}=\varphi_n(\varphi_{\xi_1}(z))$$
    При этом $\xi_1$ может быть заменена на любую из $\xi_i$, так как они одинаково распределены.
\end{definition}
\begin{example}[Деление клетки 2]\label{Деление клетки 2}
    Продолжим рассмотрение примера \ref{Деление клетки 1}.
    $$F(n;z)\defeq\E(z^{Z_{(n)}})=\sum_{k=0}^{\infty}z^k\prob(Z(n)=k)$$
    Рассмотрим $F(n+1;z)$ при учёте, что $Z(0)=1$ и используя свойства итераций:
    \begin{multline*}
        F(n+1;z)=\E\left(z^{Z_{(n+1)}}\right)=\E\left(\E\left(z^{Z_{(n+1)}}|Z(n)\right)\right)=\\
        =\E\left(\E\left(z^{\xi_1^{(n)}+\xi_2^{(n)}+\ldots+\xi_{Z(n)}^{(n)}}|Z(n)\right)\right)=\E\left(\prod_{k=1}^{Z_{(n)}}\E\left(z^{\xi_k^{(n)}}\right)\right)=\E\left(\E\left(z^\xi\right)^{Z(n)}\right)=\\
        =F(n;\varphi(z))=F(n-1;\varphi_2(z))=F(0;\varphi_{n+1}(z))=\varphi_{n+1}(z)
    \end{multline*}
    Тогда вероятность вырождения к моменту $n$ может быть вычислена как итерация:
    $$\varphi_n(0)=F(n;0)=\sum_{k=0}^{\infty}0^k\prob(Z(n)=k)=\prob(Z(n)=0)$$
\end{example}

\subsection{Классификация процессов Гальтона-Ватсона}

\begin{definition}
    Пусть $A\defeq \E(\xi)=\E(Z(1)|Z(0)=1)<\infty$. Тогда процесс Гальтона-Ватсона называется:
    \begin{center}
        \begin{multicols}{3}
            Докритическим, если $A<1$\\
            Критическим, если $A=1$\\
            Надкритическим, если $A>1$
        \end{multicols}
    \end{center}
\end{definition}

\begin{lemma}
    $\E(Z(n))=A^n$. При этом если $\D(\xi)<\infty$, то:
    $$\D(Z(n))=\left\{\begin{alignedat}{2}
        & \D\left(\frac{A^{n-1}(A^n-1)}{A-1}\right), && \text{ если }A\neq1 \\
        & \sigma(n^2),  && \text{ если }A=1
      \end{alignedat}\right.$$
\end{lemma}
\begin{proof}
    \begin{multline*}
        \E(Z(n))=\E\left(\xi_1^{(n-1)}+\xi_2^{(n-1)}+\ldots+\xi_{Z(n-1)}^{(n-1)}\right)=\E(\xi)\cdot\E(Z(n-1))=\\
        =A\cdot\E(Z(n-1))=A^2\cdot\E(Z(n-2))=\ldots=A^n\cdot\E(Z(0))=A^n
    \end{multline*}
\end{proof}

\begin{theorem}
    Вероятность вырождения ветвящегося процесса равна наименьшему неотрицательному корню $p$ уравнения $z=\varphi(z)$.
\end{theorem}
\begin{proof}
    $$\varphi_n(z)=\prob(Z(n)=0)\leq \prob(Z(n+1)=0)=\varphi_{n+1}(z)$$
    В силу монотонности и возрастания последовательности
    $$P(n)=\prob(Z(n)=0)=\varphi_n(0)$$
    выполняется теорема Вейерштрасса, по которой она имеет предел $r$ при $n\to\infty$. Заметим, что 
    $$P(1)=\varphi_1(0)=\varphi_1(\varphi_0(0))\leq p=\varphi(p)$$
    Пусть $\varphi_{n-1}(0)=P(n-1)<p$, тогда по индукции получаем:
    $$P(n)=\varphi_n(0)=\varphi(\varphi_{n-1}(0))=\varphi(P(n-1))\leq \varphi(p)=p$$
    Отсюда в силу непрерывности $\varphi$ имеем:
    $$r=\lim_{n\to\infty}P(n)=\lim_{n\to\infty}\varphi(P(n-1))=\varphi\left(\lim_{n\to\infty}P(n-1)\right)=\varphi(r)$$
    Таким образом, мы также получаем, что докритические процессы вырождаются с вероятностью $p=1$, а надкритические процессы вырождаются с вероятностью $p<1$, которая является минимальным неотрицательным корнем уравнения $z=\varphi(z)$.
\end{proof}
\begin{example}
    Рассмотрим бинарное деление клетки с производящей функцией $\varphi(z)=1-p+pz^2$. Решим уравнение $z=1-p+pz^2$, получив корни $z=1$ и $z=\dfrac{1-p}{p}$. То есть при $p>\num{0.5}$ вероятность вырождения равна $\prob=\dfrac{1-p}{p}$, а при $p\leq\num{0.5}$ имеем $\prob=1$.
\end{example}

\section{Математическая статистика}

\begin{definition}
    Функция распределения случайной величины $\xi$ суть $F_\xi(x)=\prob(\xi\leq x)$.
\end{definition}
\begin{definition}
    Квантилем уровня $\alpha$ называется такой $x_\alpha$, что он является решением уравнения $F_\xi(x)=\alpha$.
\end{definition}
\begin{definition}\label{выборка}
    Выборкой объема $n;\,\,n>1$ называется случайный вектор $Z_n=(\xi_1,\ldots,\xi_n)$, где $\xi_i,\,\,i\in\{1,\ldots,n\}$ называются элементами выборки и являются независимыми случайными величинами с одной функцией распределения $F_\xi(x)$. Тогда выборка $Z_n$ соответствует функции распределения $F_\xi(x)$.
\end{definition}
\begin{definition}\label{реализация выборки}
    Реализацией выборки называется неслучайный вектор $z_n=(x_1,\ldots,x_n)$, компонентами которого являются реализации соответствующих элементов выборки $\xi_i$, $i\in\{1,\ldots,n\}$.
\end{definition}
\begin{consequence}
    Из определений \ref{выборка} и \ref{реализация выборки} следует, что реализацию выборки $z_n$ можно также рассматривать как последовательность $x_1,\ldots,x_n$ из $n$ реализаций одной и той же случайной величины $\xi$, полученных в серии из $n$ независимых одинаковых опытов, проводимых в одинаковых условиях. Поэтому можно говорить, что выборка $Z_n$ порождена наблюдаемой случайной величиной $\xi$, имеющей распределение $F_\xi(x)=F(x)$.
\end{consequence}

\subsection{Нормальное распределение}

\begin{definition}
    Случайная величина $\xi$ распределена нормально с параметрами $\mu$ и $\sigma$, если её функция плотности вероятности имеет вид:
    $$F_\xi(x)=\frac{1}{\sigma\sqrt{2\pi}}\cdot e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$
\end{definition}
\begin{theorem}
    Пусть $\xi\sim\mathcal{N}(\mu;\,\,\sigma)$. Тогда $\E(\xi)=\mu$ и $\D(\xi)=\sigma^2$.
\end{theorem}
\begin{definition}
    Случайна величина $\xi$ распределена стандартно нормально, если $\xi\sim\nd(0;\,1)$.
\end{definition}
\begin{definition}
    Функция плотности стандартно нормально распределенной случайной величины $\xi$ обозначается:
    $$\varphi_\xi(x)=\frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{x^2}{2}}$$
\end{definition}
\begin{theorem}
    Функцией распределения стандартно нормально распределенной случайной величины $\xi$ является функция Лапласа:
    $$\Phi_\xi(x)=\int_{-\infty}^{x}\varphi_\xi(y)\, dy $$
\end{theorem}
\begin{theorem}[Правило трёх сигм]
    Для случайной величины $\xi$, имеющей стандартное нормальное распределение:
    $$\prob(|\xi-\mu|<3\sigma)\approx\num{0.9973}$$
\end{theorem}
\begin{theorem}[Центральная предельная теорема]
    Пусть $\xi_1,\ldots,\xi_n$ – последовательность независимых случайных величин, а $S_n$ – их сумма, тогда:
    $$\frac{S_n-n\cdot \E(\xi)}{\sigma(\xi)\cdot\sqrt{n}}\xrightarrow{n\to\infty}\nd(0;\,1)$$
    Пусть $\hat{\xi}$ – центрированная и нормированная случайная величина, тогда: 
    $$\prob(a<\hat{\xi}<b)=\prob\left(a<\frac{S_n-n\cdot \E(\xi)}{\sigma(\xi)\cdot\sqrt{n}}<b\right)\approx\Phi(b)-\Phi(a)$$
\end{theorem}
\begin{theorem}[Муавр-Лаплас]
    Пусть $\xi\sim B(p;\,n),\,\,n\geq30$, тогда можно сказать, что $\xi\sim\nd(np;\,npq)$, а значит:
    $$\prob(a<\xi<b)=\prob\left(\frac{a-np}{\sqrt{npq}}<\frac{\xi-np}{\sqrt{npq}}<\frac{b-np}{\sqrt{npq}}\right)=\Phi\left(\frac{b-np}{\sqrt{npq}}\right)-\Phi\left(\frac{a-np}{\sqrt{npq}}\right)$$
\end{theorem}

\subsection{Оценка выборки}

\begin{definition}
    Параметром распределения $\theta$ случайной величины $\xi$ называется любая числовая характеристика этой случайной величины или любая константа, явно входящая в выражение для функции распределения.
\end{definition}
\begin{definition}
    Случайная величина $T=\varphi(Z_n)$, где $\varphi(Z_n)$ – произвольная функция определенная на выборочном пространстве, называется статистикой.
\end{definition}
\begin{definition}
    Статистической гипотезой $H$ называется любое предположение относительно вида распределения, параметров распределения или свойств закона распределения наблюдаемой в эксперименте случайной величины $X$.
\end{definition}
\begin{definition}
    Любое предположение относительно параметров распределения случайной величины $X$ называют параметрической гипотезой.
\end{definition}
\begin{definition}
    Проверяемая гипотеза называется основной (или нулевой) и обозначается $H_0$. Гипотеза, конкурирующая с $H_0$, называется альтернативной и обозначается $H_1$.
\end{definition}
\begin{definition}
    Статистическим критерием проверки гипотезы $H_0$ называется правило, в соответствии с которым по реализации $t=\varphi(z_n)$ статистики $T$ гипотеза $H_0$ принимается или отвергается.
\end{definition}
\begin{definition}
    Критической областью $\overline{G}$ статистического критерия называется область реализаций $t$ статистики $T$, при которых гипотеза $H_0$ отвергается.
\end{definition}
\begin{definition}
    Доверительной областью $G$ статистического критерия называется область реализаций $t$ статистики $T$, при которых гипотеза $H_0$ принимается.
\end{definition}
\begin{definition}
    Ошибкой 1-го рода называется событие, состоящее в том, что гипотеза $H_0$ отвергается, когда она верна.
\end{definition}
\begin{definition}
    Ошибкой 2-го рода называется событие, состоящее в том, что принимается гипотеза $H_0$, когда верна гипотеза $H_1$.
\end{definition}
\begin{definition}
    Уровнем значимости статистического критерия называется вероятность ошибки 1-го рода:
    $$\alpha=\prob(T\in \overline{G}|H_0\text{ – верна})$$
\end{definition}
\begin{definition}
    Минимальный возможный уровень значимости обозначается $p-value$.
\end{definition}
\begin{definition}
    Точечной (выборочной) оценкой неизвестного параметра распределения $\theta$ называется произвольная статистика $\hat{\theta}(Z_n)$, построенная по выборке $Z_n$.
\end{definition}
\begin{definition}
    Оценка $\hat{\theta}$ называется несмещённой оценкой параметра $\theta$, если математическое ожидание оценки равно оцениваемому параметру, то есть $\E(\hat{\theta})=\theta$.
\end{definition}
\begin{definition}
    Оценка $\hat{\theta}$ называется состоятельной оценкой параметра $\theta$, если она сходится по вероятности к оцениваемому параметру, то есть:
    $$\prob(|\hat{\theta}_n-\theta|<\varepsilon)\xrightarrow{n\to\infty}1$$
\end{definition}
\begin{theorem}
    Размер выборки, необходимой для оценки, с точностью $\delta$ и уровнем доверия $1-\alpha$ можно выразить как:
    $$n=\left(\frac{z_{1-\frac{\alpha}{2}}}{\delta}\right)^2pq$$
\end{theorem}
\begin{proof}
    Пусть $\hat{p}$ – вероятность наличия у объекта выборки какого-то признака, тогда должно выполняться условие:
    $$\prob(|\hat{p}-p|<\delta)=1-\alpha$$
    Поскольку частота является несмещённой и состоятельной оценкой вероятности, при проведении $n$ опытов, в $X$ из которых произойдёт нужное нам событие, вероятность можно будет представить, как $\hat{p}=\frac{X}{n}$. При этом в случае выбора с возвращением, $X$ будет иметь биномиальное распределение с параметрами $n$ и $p$, где $n$ – количество проведенных экспериментов, а $p$ – вероятность появления признака. Тогда по центральной предельной теореме:
    $$\frac{X-\E(X)}{\sqrt{\D(X)}}=\frac{X-np}{\sqrt{npq}}\xrightarrow{n\to\infty}\nd(0;\,1)$$
    В таком случае можно считать, что $\hat{p}\sim\nd(0;\,1)$, а значит:
    \begin{center}
        \begin{multicols}{2}
            \[\E(\hat{p})=\E\left(\frac{X}{n}\right)=\frac{np}{n}=p\]\\
            \[\D(\hat{p})=\D\left(\frac{X}{n}\right)=\frac{npq}{n^2}=\frac{pq}{n}\]
        \end{multicols}
    \end{center}
    Зная распределение $\hat{p}$, мы можем установить, когда выполняется равенство:
    \begin{multline*}
        \prob(|\hat{p}-p|<\delta)=\prob(p-\delta<\hat{p}<p+\delta)=\Phi\left(\frac{p+\delta-p}{\sqrt{\frac{pq}{n}}}\right) - \Phi\left(\frac{p-\delta-p}{\sqrt{\frac{pq}{n}}}\right)=\\
        =\Phi\left(\frac{\delta}{\sqrt{\frac{pq}{n}}}\right) - \left(1 - \Phi\left(\frac{\delta}{\sqrt{\frac{pq}{n}}}\right)\right)=2\Phi\left(\frac{\delta}{\sqrt{\frac{pq}{n}}}\right)-1=1-\alpha\Longrightarrow \Phi\left(\frac{\delta}{\sqrt{\frac{pq}{n}}}\right) = 1-\frac{\alpha}{2}
    \end{multline*}
    Получили, что аргумент функции Лапласа является квантилем уровня $1-\dfrac{\alpha}{2}$ стандартного нормального распределения, тогда остается выразить $n$:
    $$n=\left(\frac{z_{1-\frac{\alpha}{2}}}{\delta}\right)^2pq$$
    Здесь $p$ определяется либо из предварительного опроса, либо подбирается такое её значение, при котором $\D(\hat{p})$ максимальное, то есть $p=\num{0.5}$.
\end{proof}

\subsection{Метод моментов}

\begin{definition}
    $k$–м моментом случайной величины $\xi$ называется $\E(\xi^k)$.
\end{definition}
\begin{theorem}
    Точечно оценить параметр $\theta$ можно, решив систему уравнений вида $\E(\xi^k)=\overline{X^k}$. В силу несмещённости оценок, если $\E(\xi^k)=\theta$, то $\hat\theta=\overline{X^k}$.
\end{theorem}
\begin{example}
    Пусть $X\sim U[a;\,b],\,X=\{2;3;7;9;4;13;-1\}$. Чтобы найти точечные оценки $a$ и $b$ нужно решить систему:
    $$\begin{cases}
        \overline{X}=\E(X)=\dfrac{37}{7}=\dfrac{a+b}{2}\\[2\jot]
        \overline{X^2}=\E(X^2)=47=\dfrac{a^2+ab+b^2}{3}
    \end{cases}\Longleftrightarrow\begin{cases}
        a=\dfrac{37-\sqrt{2802}}{7}\\
        b=\dfrac{37+\sqrt{2802}}{7}
    \end{cases}$$
\end{example}

\subsection{Метод максимального правдоподобия}

\begin{definition}
    Функция правдоподобия выборки $X$ суть:
    $$L(X)=\prob(x_1\cap x_2\cap\ldots\cap x_n)=\prob(x_1)\cdot\prob(x_2)\cdot\ldots\cdot\prob(x_n)$$
\end{definition}
\begin{example}
    Пусть $X\sim B(n;\,p);\,n=8;\,X=\{5;4;7;6\}$, тогда:
    $$L(X)=C_8^5p^5q^3\cdot C_8^4p^4q^4\cdot C_8^7p^7q\cdot C_8^6p^6q^2$$
\end{example}
\begin{definition}
    Логарифмическая функция правдоподобия выборки $X$ суть $\ln(L(X))$.
\end{definition}
\begin{example}
    \begin{multline*}
        \ln(L(X))=\ln(C_8^5)+\ldots+\ln(C_8^6)+\ln p^5+\ldots+\ln p^6+\ln q^3+\ldots+\ln q^2=\\
        =\ln(C_8^5)+\ldots+\ln(C_8^6)+(5+4+7+6)\cdot\ln p+(3+4+1+2)\cdot\ln q=\\
        =\ln(C_8^5)+\ldots+\ln(C_8^6)+22\ln p+10\ln(1-p)
    \end{multline*}
    $$\left(\ln(L(X))\right)'=22\cdot\dfrac{1}{p}-10\cdot\dfrac{1}{1-p}=0$$
\end{example}

\subsection{Интервальное оценивание}

\begin{statement}[Интервальная оценка в схеме испытаний Бернулли]
    Пусть в схеме испытаний Бернулли $\gamma$ – доверительная вероятность, $I_\gamma$ – доверительный интервал, $k$ – число успехов, $n$ – число реализаций, $\hat{p}=\frac{k}{n}$ – несмещённая оценка, тогда:
    $$I_\gamma=\left(\hat{p}-z_{1-\frac{\alpha}{2}}\cdot\sqrt{\frac{pq}{n}};\,\,\hat{p}+z_{1-\frac{\alpha}{2}}\cdot\sqrt{\frac{pq}{n}}\right)$$
\end{statement}

\begin{definition}[Распределение $\chi^2$]
    Случайная величина $\xi$ распределена по $\chi^2$ с $n$ степенями свободы, если:
    $$\xi=\sum_{i=1}^{n}\xi_i^2,\,\xi_i\sim\mathcal{N}(0;1)$$
\end{definition}
\begin{definition}[Распределение Стьюдента]
    Случайная величина $\xi$ распределена по $t$, если:
    $$\xi=\dfrac{\xi_0}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}\xi_i^2}},\,\,\xi_i\sim\mathcal{N}(0;1)$$
\end{definition}
\subsubsection{Интервальная оценка среднего}
\begin{statement}[Оценка среднего при известной генеральной дисперсии]\label{Оценка среднего с дисперсией}
    Пусть $\overline{x}$ – среднее выборки, $n$ – объём выборки, $\sigma$ – генеральное стандартное отклонение, тогда:
    $$I_\gamma=\left(\overline{x}-z_{1-\frac{\alpha}{2}}\cdot\frac{\sigma}{\sqrt{n}};\,\,\overline{x}+z_{1-\frac{\alpha}{2}}\cdot\frac{\sigma}{\sqrt{n}}\right)$$
\end{statement}
\begin{statement}[Оценка среднего при $n\geq30$]
    Аналогично утверждению \ref{Оценка среднего с дисперсией}, но вместо $\sigma$ используется $s$:
    $$s=\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2}$$
\end{statement}
\renewcommand{\arraystretch}{1.5}
\begin{statement}[Оценка среднего при $n<30$]
    Пусть $t_{\frac{\alpha}{2}}(n-1)$ – квантиль распределения Стьюдента с $n-1$ степенью свободы, тогда:
    $$I_\gamma=\left(\overline{x}-t_{\frac{\alpha}{2}}(n-1)\cdot\frac{s}{\sqrt{n}};\,\,\overline{x}+t_{\frac{\alpha}{2}}(n-1)\cdot\frac{s}{\sqrt{n}}\right)$$
    При этом статистикой критерия будет:
    $$T(X)=\dfrac{\overline{x}-\mu}{\sigma}\cdot\sqrt{n}\sim t(n-1)$$
    А критическая область будет иметь вид:
    \begin{center}
        \begin{tabular}{c|c}
            Альтернатива & Критическая область \\
            \hline
            Двусторонняя ($H_A:\,\overline{x}\neq\mu$) & $(-\infty;-t_{\frac{\alpha}{2}}(n-1))\cup(t_{\frac{\alpha}{2}}(n-1);+\infty)$\\
            Правосторонняя ($H_A:\,\overline{x}>\mu$) & $(t_\alpha(n-1);+\infty)$\\
            Левосторонняя ($H_A:\,\overline{x}<\mu$) & $(-\infty;-t_\alpha(n-1))$
        \end{tabular}
    \end{center}
\end{statement}

\subsection{Проверка гипотез о независимости признаков}

\begin{statement}
    Пусть дана таблица смежности признаков $A_1,A_2,\ldots,A_m$ и $B_1,B_2,\ldots,B_k$:
    \begin{center}
        \begin{tabular}{c|cccc}
            & $A_1$ & $A_2$ & $\cdots$ & $A_m$\\
            \hline
            $B_1$ & $n_{11}$ & $n_{12}$ & $\cdots$ & $n_{1m}$\\
            $B_2$ & $n_{21}$ & $n_{22}$ & $\cdots$ & $n_{2m}$\\
            $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$\\
            $B_k$ & $n_{k1}$ & $n_{k2}$ & $\cdots$ & $n_{km}$\\
        \end{tabular}
    \end{center}
    Тогда если требуется проверить гипотезу о независимости $A_i$ и $B_j$, то:
    \begin{align*}
        &H_0: \text{ признаки статистически независимы.}\\
        &H_A: \text{ между признаками существует статистическая зависимость.}\\
    \end{align*}
    \\[-2\baselineskip]
    Введём $N$ – общее число наблюдений, $n_{i+}$ – сумма наблюдений по строке и $n_{+i}$ – сумма наблюдений по столбцу. Тогда таблицу можно расширить до вида:
    \begin{center}
        \begin{tabular}{c|cccc|c}
            & $A_1$ & $A_2$ & $\cdots$ & $A_m$ &\\
            \hline
            $B_1$ & $n_{11}$ & $n_{12}$ & $\cdots$ & $n_{1m}$ & $n_{1+}$\\
            $B_2$ & $n_{21}$ & $n_{22}$ & $\cdots$ & $n_{2m}$ & $n_{2+}$\\
            $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\
            $B_k$ & $n_{k1}$ & $n_{k2}$ & $\cdots$ & $n_{km}$ & $n_{k+}$\\
            \hline
            & $n_{+1}$ & $n_{+2}$ & $\cdots$ & $n_{+m}$ & $N$
        \end{tabular}
    \end{center}
    Введём $n_{ij}^*$ – ожидаемое число наблюдений признаков $A_i$ и $B_j$ одновременно при условии их независимости:
    $$n_{ij}^*=N\cdot\frac{n_{+i}}{N}\cdot\frac{n_{j+}}{N}=\frac{n_{+i}\cdot n_{j+}}{N}$$
    Отсюда статистикой критерия будет:
    $$T(X)=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(n_{ij}-n_{ij}^*)^2}{n_{ij}^*}\sim \chi^2\left((m-1)(k-1)\right)$$
    А критическая область будет иметь вид:
    $$\overline{G}=\left(\chi^2_{1-\alpha}((m-1)(k-1));+\infty\right)$$
\end{statement}

\subsubsection{Критерий согласия Пирсона}

\begin{statement}
    Пусть дано некоторое распределение и требуется проверить равенство его и распределения некоторой случайной величины. Тогда если $\hat{p_i}$ – теоретическое значение вероятности, то:
    \begin{align*}
        &H_0:\, \forall\, i\in\{1,\ldots,k\}:\,p_i=\hat{p_i}\\
        &H_A: \exists\, i\in\{1,\ldots,k\}:\,p_i\neq\hat{p_i}\\
    \end{align*}
    При этом если объём выборки равен $n$, а $n_i$ – количество реализаций $i$-го значения случайной величины, то статистикой критерия будет:
    $$T(X)=\sum_{i=1}^{k}\frac{(n_i-p_i\cdot n)^2}{p_i\cdot n}\sim \chi^2(n-1)$$
\end{statement}

\subsection{Непрерывные распределения}

\begin{definition}
    Функцией распределения случайной величины называется:
    $$F_\xi(x)=\prob(\xi\leq x)$$
\end{definition}
\begin{definition}
    Функцией плотности распределения называется:
    $$f_\xi(x)=F_\xi'(x):\,\int_{\R}f_\xi(x)=1$$
\end{definition}
\begin{definition}
    Математическим ожиданием непрерывно распределённой случайной величины называется:
    $$\E(\xi)=\int_{-\infty}^{\infty}x\cdot f_\xi(x)\diff x$$
\end{definition}

\subsubsection{Экспоненциальное распределение}

\begin{definition}
    Случайная величина $\tau$ распределена показательно (экспоненциально), если она моделирует ожидание следующего успеха в Пуассоновском процессе.
\end{definition}
\begin{definition}
    Функцией плотности экспоненциально распределённой случайной величины называется:
    $$f_\tau(t)=\begin{cases}
        \lambda e^{-\lambda t},\,\,t\geq 0\\
        0,\,\,t<0
    \end{cases}$$
\end{definition}
\begin{statement}
    Пусть $\tau\sim Exp(\lambda)$. Тогда:
    $$F_\tau(t)=\begin{cases}
        1-e^{-\lambda t},\,\,t\geq 0\\
        0,\,\,t<0
    \end{cases}$$
\end{statement}
\begin{proof}
    $$F_\tau(t)=\int_{0}^{t}\lambda e^{-\lambda \hat{t}}\diff \hat{t}=\begin{vmatrix}
    x=-\lambda t\\
    \diff x=-\lambda\diff \hat{t}
    \end{vmatrix}=-\int_{0}^{-\lambda t}e^x\diff x=-e^x\Big|_0^{-\lambda t}=1-e^{-\lambda t}$$
\end{proof}
\begin{statement}
    Пусть $\tau\sim Exp(\lambda)$. Тогда:
    $$\E(\tau)=\frac{1}{\lambda};\,\,\,\D(\tau)=\frac{1}{\lambda^2}$$
\end{statement}
\begin{statement}[Независимость от приращений]
    Пусть $\tau\sim Exp(\lambda)$. Тогда:
    $$\prob(\tau>t_1+t_2,\,|\,\tau>t_1)=\prob(\tau>t_2)$$
\end{statement}
\begin{proof}
    \begin{multline*}
        \prob(\tau>t_1+t_2,\,|\,\tau>t_1)=\frac{\prob(\tau>t_1+t_2)}{\prob(\tau>t_1)}=\\=\frac{1-\prob(\tau\leq t_1+t_2)}{1-\prob(\tau\leq t_1)}=\frac{1-(1-e^{-\lambda(t_1+t_2)})}{1-(1-e^{-\lambda t_1})}=\frac{e^{-\lambda(t_1+t_2)}}{e^{-\lambda t_1}}=\\=e^{-\lambda t_2}=1-(1-e^{-\lambda t_2})=\prob(\tau>t_2)
    \end{multline*}
\end{proof}
\end{document}